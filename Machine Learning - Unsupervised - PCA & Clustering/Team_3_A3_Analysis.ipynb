{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                            #Importing Packages\n",
    "##############################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                            #User Defined Functions\n",
    "##############################################################################\n",
    "\n",
    "########################################\n",
    "# inertia\n",
    "########################################\n",
    "def inertia_plot(data, max_clust = 50):\n",
    "    \"\"\"\n",
    "PARAMETERS\n",
    "----------\n",
    "data      : DataFrame, data from which to build clusters. Dataset should be scaled\n",
    "max_clust : int, maximum of range for how many clusters to check interia, default 50\n",
    "    \"\"\"\n",
    "\n",
    "    ks = range(1, max_clust)\n",
    "    inertias = []\n",
    "\n",
    "\n",
    "    for k in ks:\n",
    "        # INSTANTIATING a kmeans object\n",
    "        model = KMeans(n_clusters = k)\n",
    "\n",
    "\n",
    "        # FITTING to the data\n",
    "        model.fit(data)\n",
    "\n",
    "\n",
    "        # append each inertia to the list of inertias\n",
    "        inertias.append(model.inertia_)\n",
    "\n",
    "\n",
    "\n",
    "    # plotting ks vs inertias\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    plt.plot(ks, inertias, '-o')\n",
    "\n",
    "\n",
    "    # labeling and displaying the plot\n",
    "    plt.xlabel('number of clusters, k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.xticks(ks)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "########################################\n",
    "# scree_plot\n",
    "########################################\n",
    "def scree_plot(pca_object, export = False):\n",
    "    # building a scree plot\n",
    "\n",
    "    # setting plot size\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    features = range(pca_object.n_components_)\n",
    "\n",
    "\n",
    "    # developing a scree plot\n",
    "    plt.plot(features,\n",
    "             pca_object.explained_variance_ratio_,\n",
    "             linewidth = 2,\n",
    "             marker = 'o',\n",
    "             markersize = 10,\n",
    "             markeredgecolor = 'black',\n",
    "             markerfacecolor = 'grey')\n",
    "\n",
    "\n",
    "    # setting more plot options\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('PCA feature')\n",
    "    plt.ylabel('Explained Variance')\n",
    "    plt.xticks(features)\n",
    "\n",
    "    if export == True:\n",
    "    \n",
    "        # exporting the plot\n",
    "        plt.savefig('top_customers_correlation_scree_plot.png')\n",
    "        \n",
    "    # displaying the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                                #Reading File\n",
    "##############################################################################\n",
    "\n",
    "survey_df = pd.read_excel('survey_data.xlsx')\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',1000)\n",
    "pd.set_option('display.max_colwidth',100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #Deleting Repeated Columns\n",
    "##############################################################################\n",
    "\n",
    "survey_df = survey_df.drop(labels=['Respond effectively to multiple priorities.1',\n",
    "                                  'Take initiative even when circumstances, objectives, or rules aren\\'t clear.1',\n",
    "                                  'Encourage direct and open discussions.1'\n",
    "                                 ],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                                #Renaming Columns\n",
    "##############################################################################\n",
    "\n",
    "survey_df.columns = ['surveyID',\n",
    "                    'life_party',\n",
    "                    'little_concern_others',\n",
    "                    'always_prepared',\n",
    "                    'stressed_easily',\n",
    "                    'rich_vocabulary',\n",
    "                    'dont_talk',\n",
    "                    'interested_people',\n",
    "                    'leave_belongings',\n",
    "                    'relaxes_most_time',\n",
    "                    'difficulty_understanding',\n",
    "                    'comfortable_people',\n",
    "                    'insult_people',\n",
    "                    'attention_details',\n",
    "                    'worry_things',\n",
    "                    'vivid_imagination',\n",
    "                    'keep_background',\n",
    "                    'sympathize_others',\n",
    "                    'make_mess',\n",
    "                    'seldom_feel_blue',\n",
    "                    'not_interest_abstract',\n",
    "                    'start_conversations',\n",
    "                    'not_interest_people',\n",
    "                    'chores_done',\n",
    "                    'easily_disturbed',\n",
    "                    'excellent_ideas',\n",
    "                    'little_say',\n",
    "                    'soft_heart',\n",
    "                    'forget_things',\n",
    "                    'upset_easily',\n",
    "                    'not_good_imagination',\n",
    "                    'talk_people',\n",
    "                    'not_interested_others',\n",
    "                    'like_order',\n",
    "                    'change_mood',\n",
    "                    'quick_understand',\n",
    "                    'draw_attention',\n",
    "                    'take_time',\n",
    "                    'shirt_duties',\n",
    "                    'mood_swings',\n",
    "                    'difficult_words',\n",
    "                    'center_of_attention',\n",
    "                    'feel_others',\n",
    "                    'follow_schedule',\n",
    "                    'irritated_easily',\n",
    "                    'time_reflecting',\n",
    "                    'quite_strangers',\n",
    "                    'people_at_ease',\n",
    "                    'exacting_word',\n",
    "                    'often_feel_blue',\n",
    "                    'full_ideas',\n",
    "                    'underlying_patterns',\n",
    "                    'no_new_ideas',\n",
    "                    'awareness',\n",
    "                    'growth_mindset',\n",
    "                    'multiple_priorities',\n",
    "                    'take_initiative',\n",
    "                    'open_discussions',\n",
    "                    'listen_carefully',\n",
    "                    'dont_sell_idea',\n",
    "                    'cooperative_relationships',\n",
    "                    'work_diverse',\n",
    "                    'effectively_negotiate',\n",
    "                    'cant_rally_people',\n",
    "                    'plans_organized',\n",
    "                    'resolve_conflicts',\n",
    "                    'seek_feedback',\n",
    "                    'coach_teammates',\n",
    "                    'drive_results',\n",
    "                    'current_laptop',\n",
    "                    'next_laptop',\n",
    "                    'program',\n",
    "                    'age',\n",
    "                    'gender',\n",
    "                    'nationality',\n",
    "                    'ethnicity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #Checking for missing values\n",
    "##############################################################################\n",
    "\n",
    "survey_df.info()\n",
    "\n",
    "survey_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #Demographic Analysis\n",
    "##############################################################################\n",
    "\n",
    "#CONVERSION RATES\n",
    "\n",
    "pd.pivot_table(survey_df,index=[\"current_laptop\",\n",
    "                                   \"next_laptop\"],\n",
    "               values=[\"surveyID\"],aggfunc='count')\n",
    "\n",
    "\n",
    "\n",
    "#CONVERSION RATES\n",
    "#1.35% of Mac users would choose Chromebook as their next laptop of choice\n",
    "#6.76% of Mac users would choose Windows as their next laptop of choice\n",
    "#91.81% of Mac users would stick to Mac\n",
    "\n",
    "\n",
    "#4.11% of Windows users would choose Chrome as their next laptop of choice\n",
    "#20.55% of Windows users would choose Mac as their next laptop of choice\n",
    "#75.34% of Windows users would stick to Windows\n",
    "\n",
    "#FEMALE - MALE Laptop of choice\n",
    "pd.pivot_table(survey_df,index=[\"gender\",\n",
    "                                   \"current_laptop\"],\n",
    "               values=[\"surveyID\"],aggfunc='count')\n",
    "\n",
    "# 58.73% Female users use Mac \n",
    "# 41.27% Female users use Windows\n",
    "\n",
    "# 44.05% Male users use Mac\n",
    "# 55.95% Male users use Windows\n",
    "\n",
    "pd.pivot_table(survey_df,index=[\"program\",\n",
    "                                   \"current_laptop\"],\n",
    "               values=[\"surveyID\"],aggfunc='count')\n",
    "\n",
    "#side note only one respondant from DD (MBA & Disruptive innovation) might be an typo\n",
    "\n",
    "#DD (MBA & Business Analytics)\n",
    "# 34.21% of DD (MIB & Business Analytics) use Mac \n",
    "# 65.78% DD (MIB & Business Analytics) use Windows\n",
    "\n",
    "#DD (MBA & Disruptive innovation)\n",
    "# 100% DD (MBA & Disruptive innovation) use Mac (1 person)\n",
    "\n",
    "#DD (MIB & Business Analytics)\n",
    "# 63.77% of DD (MIB & Business Analytics) use Mac\n",
    "# 36,.23% of DD (MIB & Business Analytics) use Windows\n",
    "\n",
    "#One year Business Analytics\n",
    "# 41.03% of One year Business Analytics use Mac\n",
    "# 58.97% of One year Business Analytics use Windows\n",
    "\n",
    "pd.pivot_table(survey_df,index=[\"ethnicity\",\n",
    "                                   \"current_laptop\"],\n",
    "               values=[\"surveyID\"],aggfunc='count')\n",
    "\n",
    "\n",
    "#Laptop users by Ethnicity \n",
    "\n",
    "#African American 9 people\n",
    "# 55.56% use Mac \n",
    "# 44.44% use Windows\n",
    "\n",
    "#Far east Asian 31 people\n",
    "# 58.06% use Mac \n",
    "# 41.93% use Windows\n",
    "\n",
    "#Hispanic / Latino 26 people\n",
    "# 26.92% use Mac \n",
    "# 73.08% use Windows\n",
    "\n",
    "#Middle Eastern 3 people\n",
    "# 33.33% use Mac \n",
    "# 66.67% use Windows\n",
    "\n",
    "#Native American 1 person\n",
    "# 100% use Mac \n",
    "\n",
    "#Prefer not to answer 11 people\n",
    "# 54.55% use Mac \n",
    "# 45.45% use Windows\n",
    "\n",
    "#West Asian / Indian 26 people\n",
    "# 38.46% use Mac \n",
    "# 61.54% use Windows\n",
    "\n",
    "#White / Caucasian 40 people\n",
    "# 65.00% use Mac \n",
    "# 35.00% use Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Applying 5 Personality Traits Formula\n",
    "##############################################################################\n",
    "\n",
    "extroversion=[]\n",
    "agreeableness=[]\n",
    "conscientiousness=[]\n",
    "neuroticism=[]\n",
    "openness=[]\n",
    "\n",
    "for i in range(0,len(survey_df)):\n",
    "    extroversion.append(20)\n",
    "    agreeableness.append(14)\n",
    "    conscientiousness.append(14)\n",
    "    neuroticism.append(38)\n",
    "    openness.append(8)\n",
    "\n",
    "for index,col in survey_df.iterrows():\n",
    "    extroversion[index]=extroversion[index]+survey_df.iloc[index,1]-\\\n",
    "                        survey_df.iloc[index,6]+survey_df.iloc[index,11]-\\\n",
    "                        survey_df.iloc[index,16]+survey_df.iloc[index,21]-\\\n",
    "                        survey_df.iloc[index,26]+survey_df.iloc[index,31]-\\\n",
    "                        survey_df.iloc[index,36]+survey_df.iloc[index,41]-\\\n",
    "                        survey_df.iloc[index,46]\n",
    "    \n",
    "    agreeableness[index]=agreeableness[index]-survey_df.iloc[index,2]+\\\n",
    "                        survey_df.iloc[index,7]-survey_df.iloc[index,12]+\\\n",
    "                        survey_df.iloc[index,17]-survey_df.iloc[index,22]+\\\n",
    "                        survey_df.iloc[index,27]-survey_df.iloc[index,32]+\\\n",
    "                        survey_df.iloc[index,37]+survey_df.iloc[index,42]+\\\n",
    "                        survey_df.iloc[index,47]\n",
    "    \n",
    "    conscientiousness[index]=conscientiousness[index]+survey_df.iloc[index,3]-\\\n",
    "                        survey_df.iloc[index,8]+survey_df.iloc[index,13]-\\\n",
    "                        survey_df.iloc[index,18]+survey_df.iloc[index,23]-\\\n",
    "                        survey_df.iloc[index,28]+survey_df.iloc[index,33]-\\\n",
    "                        survey_df.iloc[index,38]+survey_df.iloc[index,43]+\\\n",
    "                        survey_df.iloc[index,48]\n",
    "    \n",
    "    neuroticism[index]=neuroticism[index]-survey_df.iloc[index,4]+\\\n",
    "                        survey_df.iloc[index,9]-survey_df.iloc[index,14]+\\\n",
    "                        survey_df.iloc[index,19]-survey_df.iloc[index,24]-\\\n",
    "                        survey_df.iloc[index,29]-survey_df.iloc[index,34]-\\\n",
    "                        survey_df.iloc[index,39]-survey_df.iloc[index,44]-\\\n",
    "                        survey_df.iloc[index,49]\n",
    "    \n",
    "    openness[index]=openness[index]+survey_df.iloc[index,5]-\\\n",
    "                        survey_df.iloc[index,10]+survey_df.iloc[index,15]-\\\n",
    "                        survey_df.iloc[index,20]+survey_df.iloc[index,25]-\\\n",
    "                        survey_df.iloc[index,30]+survey_df.iloc[index,35]+\\\n",
    "                        survey_df.iloc[index,40]+survey_df.iloc[index,45]+\\\n",
    "                        survey_df.iloc[index,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Concatenating New Columns to DF\n",
    "##############################################################################\n",
    "\n",
    "extroversion = pd.DataFrame(extroversion)\n",
    "extroversion.columns = ['extroversion']\n",
    "\n",
    "agreeableness = pd.DataFrame(agreeableness)\n",
    "agreeableness.columns = ['agreeableness']\n",
    "\n",
    "conscientiousness = pd.DataFrame(conscientiousness)\n",
    "conscientiousness.columns = ['conscientiousness']\n",
    "\n",
    "neuroticism = pd.DataFrame(neuroticism)\n",
    "neuroticism.columns = ['neuroticism']\n",
    "\n",
    "openness = pd.DataFrame(openness)\n",
    "openness.columns = ['openness']\n",
    "\n",
    "survey_df_processed = pd.concat([survey_df,extroversion,agreeableness,conscientiousness,neuroticism,openness],\n",
    "                                axis=1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #Deleting Pyschometrics\n",
    "##############################################################################\n",
    "#We will delete the first 50 columns corresponding to the psychometrics\n",
    "#because we have summarized them using the formula for the Big 5 Traits\n",
    "\n",
    "deleted_columns=[]\n",
    "for i in range(1,51):\n",
    "    deleted_columns.append(survey_df_processed.columns[i])\n",
    "\n",
    "survey_df_processed = survey_df_processed.drop(labels=deleted_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                            #Separating Columns\n",
    "##############################################################################\n",
    "\n",
    "#Separating Demographic Data\n",
    "\n",
    "survey_demographic = survey_df_processed.iloc[:,[0,19,20,21,22,23,24,25]]\n",
    "\n",
    "#Separating Psychometric Data\n",
    "\n",
    "survey_psychometric = survey_df_processed.iloc[:,26:]\n",
    "\n",
    "#Separating Hult DNA Data\n",
    "\n",
    "survey_hult = survey_df_processed.iloc[:,1:19]\n",
    "\n",
    "survey_demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                     #Standardizing Data for Psychometrics\n",
    "##############################################################################\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(survey_psychometric)\n",
    "big5_scaled = scaler.transform(survey_psychometric)\n",
    "big5_scaled_df = pd.DataFrame(big5_scaled) \n",
    "big5_scaled_df.columns = survey_psychometric.columns\n",
    "\n",
    "# checking pre- and post-scaling variance\n",
    "print(pd.np.var(survey_psychometric), '\\n\\n')\n",
    "print(pd.np.var(big5_scaled_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Creating PCA Model for Psychometrics\n",
    "##############################################################################\n",
    "\n",
    "# INSTANTIATING a PCA object with no limit to principal components\n",
    "pca = PCA(n_components = None,\n",
    "          random_state = 802)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the scaled data\n",
    "customer_big5 = pca.fit_transform(big5_scaled_df)\n",
    "\n",
    "\n",
    "# calling the scree_plot function\n",
    "scree_plot(pca_object = pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "    We will use 3 features for the PCA model because they explain most of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                #PCA Model with 3 Components for Psychometrics\n",
    "##############################################################################\n",
    "\n",
    "# INSTANTIATING a new model using the first three principal components\n",
    "new_pca = PCA(n_components = 3,\n",
    "            random_state = 802)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the purchases_scaled\n",
    "customer_new_big5 = new_pca.fit_transform(big5_scaled_df)\n",
    "\n",
    "\n",
    "# calling the scree_plot function\n",
    "scree_plot(pca_object = new_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                #Factor Loadings Analysis for Psychometrics\n",
    "##############################################################################\n",
    "\n",
    "####################\n",
    "### Max PC Model ###\n",
    "####################\n",
    "# transposing pca components (pc = MAX)\n",
    "factor_loadings = pd.DataFrame(pd.np.transpose(pca.components_))\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings = factor_loadings.set_index(big5_scaled_df.columns)\n",
    "\n",
    "\n",
    "####################\n",
    "### New PC Model ###\n",
    "####################\n",
    "# transposing pca components (pc = 3)\n",
    "factor_loadings_new = pd.DataFrame(pd.np.transpose(new_pca.components_))\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings_new= factor_loadings_new.set_index(big5_scaled_df.columns)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "print(f\"\"\"\n",
    "MAX Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings.round(2)}\n",
    "\n",
    "3 Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings_new.round(2)}\n",
    "\"\"\")\n",
    "\n",
    "# checking the result\n",
    "factor_loadings_new\n",
    "\n",
    "# analyzing factor strengths per customer\n",
    "X_pca_reduced = new_pca.transform(big5_scaled_df)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_pca_df = pd.DataFrame(X_pca_reduced)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "X_pca_df\n",
    "\n",
    "pd.np.var(X_pca_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor 0: It's low on everything, specially in agreeableness and conscientiouness, so we will call them Shy <br>\n",
    "Factor 1: It's high on extroversion but also in neurotism, this means he/she cannot control too much their emotion. We will call them Uproared <br>\n",
    "Factor 2: It's very agreeable and open, with a good control of emotions (low neurotism). We will called them Balanced <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Naming Components for Psychometrics\n",
    "##############################################################################\n",
    "\n",
    "factor_loadings_new.columns = ['Shy',\n",
    "                               'Uproared',\n",
    "                               'Balanced']\n",
    "\n",
    "factor_loadings_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Converting Psychometrics to Factors\n",
    "##############################################################################\n",
    "\n",
    "survey_big5 = new_pca.transform(big5_scaled_df)\n",
    "\n",
    "survey_big5_df = pd.DataFrame(survey_big5)\n",
    "\n",
    "survey_big5_df.columns = factor_loadings_new.columns\n",
    "\n",
    "survey_big5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #Arranging Data for Hult DNA\n",
    "##############################################################################\n",
    "\n",
    "#Aligning values for negative columns\n",
    "\n",
    "#Grabbing the negative columns\n",
    "hult_inverse = survey_hult.loc[:,['no_new_ideas','dont_sell_idea','cant_rally_people']]\n",
    "\n",
    "#Inverting Values\n",
    "for i in range(0,3):\n",
    "    hult_inverse.iloc[:,i] = 6 - hult_inverse.iloc[:,i]\n",
    "    \n",
    "#Chaning names\n",
    "hult_inverse.columns = ['new_ideas','sell_ideas','rally_people']\n",
    "\n",
    "#Adding inverse columns and droping old ones\n",
    "survey_hult_processed = survey_hult.drop(['no_new_ideas','dont_sell_idea','cant_rally_people'],axis=1)\n",
    "survey_hult_processed = pd.concat([survey_hult_processed,hult_inverse],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #Standardizing Data for Hult DNA\n",
    "##############################################################################\n",
    "# scaling the data\n",
    "scaler_hult = StandardScaler()\n",
    "\n",
    "scaler_hult.fit(survey_hult_processed)\n",
    "hult_scaled = scaler_hult.transform(survey_hult_processed)\n",
    "hult_scaled_df = pd.DataFrame(hult_scaled) \n",
    "hult_scaled_df.columns = survey_hult_processed.columns\n",
    "\n",
    "# checking pre- and post-scaling variance\n",
    "print(pd.np.var(survey_hult), '\\n\\n')\n",
    "print(pd.np.var(hult_scaled_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Creating PCA Model for Hult DNA\n",
    "##############################################################################\n",
    "\n",
    "# INSTANTIATING a PCA object with no limit to principal components\n",
    "pca_hult = PCA(n_components = None,\n",
    "          random_state = 802)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the scaled data\n",
    "customer_hult = pca_hult.fit_transform(hult_scaled_df)\n",
    "\n",
    "\n",
    "# calling the scree_plot function\n",
    "scree_plot(pca_object = pca_hult)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 3 components because they explain most of the variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                #PCA Model with 3 Components for Hult DNA\n",
    "##############################################################################\n",
    "\n",
    "# INSTANTIATING a new model using the first three principal components\n",
    "new_pca_hult = PCA(n_components = 3,\n",
    "            random_state = 802)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the purchases_scaled\n",
    "customer_new_hult = new_pca_hult.fit_transform(hult_scaled_df)\n",
    "\n",
    "# calling the scree_plot function\n",
    "scree_plot(pca_object = new_pca_hult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                #Factor Loadings Analysis for Hult DNA\n",
    "##############################################################################\n",
    "\n",
    "####################\n",
    "### Max PC Model ###\n",
    "####################\n",
    "# transposing pca components (pc = MAX)\n",
    "factor_loadings = pd.DataFrame(pd.np.transpose(pca_hult.components_))\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings = factor_loadings.set_index(hult_scaled_df.columns)\n",
    "\n",
    "\n",
    "####################\n",
    "### New PC Model ###\n",
    "####################\n",
    "# transposing pca components (pc = 3)\n",
    "factor_loadings_new = pd.DataFrame(pd.np.transpose(new_pca_hult.components_))\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings_new= factor_loadings_new.set_index(hult_scaled_df.columns)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "print(f\"\"\"\n",
    "MAX Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings.round(2)}\n",
    "\n",
    "3 Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings_new.round(2)}\n",
    "\"\"\")\n",
    "\n",
    "# checking the result\n",
    "factor_loadings_new\n",
    "\n",
    "# analyzing factor strengths per customer\n",
    "X_pca_reduced = new_pca_hult.transform(hult_scaled_df)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_pca_df = pd.DataFrame(X_pca_reduced)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "X_pca_df\n",
    "\n",
    "pd.np.var(X_pca_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor 0: We can see the first component ranks negative in all elements of Hult DNA so we will name them Beginners <br>\n",
    "Factor 1: We can see the second component is high on team building skills so we will name them TeamPlayers <br>\n",
    "Factor 2: We can see the third component is high in the awareness skill so we will name the GrowthMindset <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Naming Components for Hult DNA\n",
    "##############################################################################\n",
    "\n",
    "factor_loadings_new.columns = ['Beginners',\n",
    "                               'TeamPlayers',\n",
    "                               'GrowthMindset']\n",
    "\n",
    "factor_loadings_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Converting Hult DNA Data to Factors\n",
    "##############################################################################\n",
    "\n",
    "survey_hult_dna = new_pca_hult.transform(hult_scaled_df)\n",
    "\n",
    "survey_hult_dna_df = pd.DataFrame(survey_hult_dna)\n",
    "\n",
    "survey_hult_dna_df.columns = factor_loadings_new.columns\n",
    "\n",
    "survey_hult_dna_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                #Concatenating Data to Standardize and Cluster\n",
    "##############################################################################\n",
    "\n",
    "factorized_survey = pd.concat([survey_big5_df,survey_hult_dna_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Standardizing Factorized Data\n",
    "##############################################################################\n",
    "\n",
    "# scaling the data\n",
    "scaler_factorized = StandardScaler()\n",
    "\n",
    "scaler_factorized.fit(factorized_survey)\n",
    "factorized_scaled = scaler_factorized.transform(factorized_survey)\n",
    "factorized_scaled_df = pd.DataFrame(factorized_scaled) \n",
    "factorized_scaled_df.columns = factorized_survey.columns\n",
    "\n",
    "# checking pre- and post-scaling variance\n",
    "print(pd.np.var(factorized_survey), '\\n\\n')\n",
    "print(pd.np.var(factorized_scaled_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                    #Dendrogram and Inertia Plot\n",
    "##############################################################################\n",
    "\n",
    "# grouping data based on Ward distance\n",
    "standard_mergings_ward = linkage(y = factorized_scaled_df,\n",
    "                                 method = 'ward')\n",
    "\n",
    "\n",
    "# setting plot size\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# developing a dendrogram\n",
    "dendrogram(Z = standard_mergings_ward,\n",
    "           leaf_rotation = 90,\n",
    "           leaf_font_size = 6)\n",
    "\n",
    "\n",
    "# saving and displaying the plot\n",
    "plt.savefig('standard_hierarchical_clust_ward.png')\n",
    "plt.show()\n",
    "\n",
    "#Running Intertia Plot\n",
    "\n",
    "inertia_plot(data = factorized_scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dendrogram and the inertia plot we believe the optimal number of cluster will be 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #K-Means Model\n",
    "##############################################################################\n",
    "\n",
    "# INSTANTIATING a k-Means object with five clusters\n",
    "customers_k_pca = KMeans(n_clusters = 3,\n",
    "                        random_state = 802)\n",
    "\n",
    "\n",
    "# fitting the object to the data\n",
    "customers_k_pca.fit(factorized_scaled_df)\n",
    "\n",
    "\n",
    "# converting the clusters to a DataFrame\n",
    "customers_kmeans_pca = pd.DataFrame({'Cluster': customers_k_pca.labels_})\n",
    "\n",
    "\n",
    "# checking the results\n",
    "print(customers_kmeans_pca.iloc[: , 0].value_counts())\n",
    "\n",
    "# storing cluster centers\n",
    "centroids_pca = customers_k_pca.cluster_centers_\n",
    "\n",
    "\n",
    "# converting cluster centers into a DataFrame\n",
    "centroids_pca_df = pd.DataFrame(centroids_pca)\n",
    "\n",
    "#Renaming columns for centroids\n",
    "centroids_pca_df.columns = factorized_scaled_df.columns\n",
    "\n",
    "# checking results (clusters = rows, pc = columns)\n",
    "print(centroids_pca_df.round(2))\n",
    "\n",
    "# concatinating cluster memberships with principal components\n",
    "clst_pca_df = pd.concat([customers_kmeans_pca,\n",
    "                          factorized_scaled_df],\n",
    "                          axis = 1)\n",
    "clst_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #Adding Demographic Data\n",
    "##############################################################################\n",
    "\n",
    "#Lowercasing demographics\n",
    "\n",
    "lower_laptop=[]\n",
    "for i,col in survey_demographic.iterrows():\n",
    "    lower_laptop.append(survey_demographic.loc[i,'current_laptop'].lower())\n",
    "\n",
    "lower_next=[]\n",
    "for i,col in survey_demographic.iterrows():\n",
    "    lower_next.append(survey_demographic.loc[i,'next_laptop'].lower())\n",
    "\n",
    "lower_program=[]\n",
    "for i,col in survey_demographic.iterrows():\n",
    "    lower_program.append(survey_demographic.loc[i,'program'].lower())\n",
    "\n",
    "lower_gender=[]\n",
    "for i,col in survey_demographic.iterrows():\n",
    "    lower_gender.append(survey_demographic.loc[i,'gender'].lower())\n",
    "\n",
    "lower_nationality=[]\n",
    "for i,col in survey_demographic.iterrows():\n",
    "    lower_nationality.append(survey_demographic.loc[i,'nationality'].lower())\n",
    "\n",
    "lower_ethnicity=[]\n",
    "for i,col in survey_demographic.iterrows():\n",
    "    lower_ethnicity.append(survey_demographic.loc[i,'ethnicity'].lower())\n",
    "\n",
    "#Converting to DataFrame    \n",
    "lower_laptop = pd.DataFrame(lower_laptop)\n",
    "lower_next = pd.DataFrame(lower_next)\n",
    "lower_program = pd.DataFrame(lower_program)\n",
    "lower_gender = pd.DataFrame(lower_gender)\n",
    "lower_nationality = pd.DataFrame(lower_nationality)\n",
    "lower_ethnicity = pd.DataFrame(lower_ethnicity)\n",
    "\n",
    "lower_laptop.columns = ['current_laptop']\n",
    "lower_next.columns = ['next_laptop']\n",
    "lower_program.columns = ['program']\n",
    "lower_gender.columns = ['gender']\n",
    "lower_nationality.columns = ['nationality']\n",
    "lower_ethnicity.columns = ['ethnicity']\n",
    "\n",
    "\n",
    "# concatenating demographic information with pca-clusters\n",
    "final_pca_clust_df = pd.concat([clst_pca_df,\n",
    "                                lower_laptop,\n",
    "                                lower_next,\n",
    "                                lower_program,\n",
    "                                lower_gender,\n",
    "                                lower_nationality,\n",
    "                                lower_ethnicity,\n",
    "                                survey_demographic['age']\n",
    "                               ],\n",
    "                                  axis = 1)\n",
    "\n",
    "final_pca_clust_df.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pca_clust_df.to_excel('clustered_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #Graphical Analysis\n",
    "##############################################################################\n",
    "\n",
    "########################\n",
    "# Gender\n",
    "########################\n",
    "\n",
    "sns.countplot(x = 'gender',\n",
    "            hue = 'Cluster',\n",
    "            data = final_pca_clust_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.countplot(x = 'Cluster',\n",
    "            hue = 'gender',\n",
    "            data = final_pca_clust_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "sns.countplot(x = 'program',\n",
    "            hue = 'Cluster',\n",
    "            data = final_pca_clust_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "sns.countplot(x = 'Cluster',\n",
    "            hue = 'program',\n",
    "            data = final_pca_clust_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "sns.countplot(x = 'ethnicity',\n",
    "            hue = 'Cluster',\n",
    "            data = final_pca_clust_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "sns.countplot(x = 'next_laptop',\n",
    "            hue = 'Cluster',\n",
    "            data = final_pca_clust_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "sns.countplot(x = 'Cluster',\n",
    "            hue = 'next_laptop',\n",
    "            data = final_pca_clust_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "sns.countplot(x = 'Cluster',\n",
    "            hue = 'current_laptop',\n",
    "            data = final_pca_clust_df)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "                        #Graphical Analysis\n",
    "##############################################################################\n",
    "\n",
    "########################\n",
    "# Gender\n",
    "########################\n",
    "\n",
    "var = ['Shy',\n",
    "        'Uproared',\n",
    "        'Balanced',\n",
    "        'Beginners',\n",
    "        'TeamPlayers',\n",
    "        'GrowthMindset']\n",
    "\n",
    "\"\"\"current_laptop\n",
    "next_laptop\n",
    "program\n",
    "gender\n",
    "nationality\n",
    "ethnicity\n",
    "age\"\"\"\n",
    "\n",
    "for col in var:\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    sns.boxplot(x = 'current_laptop',\n",
    "                y = col,\n",
    "                hue = 'Cluster',\n",
    "                data = final_pca_clust_df)\n",
    "    plt.ylim(-10, 19)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
